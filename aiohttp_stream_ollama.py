import sys

import aiohttp
import asyncio
import json
from typing import AsyncGenerator


async def stream_ollama(
        model: str = "gpt-oss:20b",
        prompt: str = "è§£é‡Šé‡å­åŠ›å­¦ä¸­çš„å åŠ åŸç†ã€‚",
        stream_url: str = "http://192.168.3.250:11434/api/generate"
) -> AsyncGenerator[str, None]:
    """
    å¼‚æ­¥æµå¼è°ƒç”¨ Ollama çš„ /api/generate æ¥å£ï¼Œé€å— yield å“åº”å†…å®¹
    """
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True,  # å…³é”®ï¼å¯ç”¨æµå¼
    }

    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(stream_url, json=payload) as resp:
                # æ£€æŸ¥ HTTP çŠ¶æ€
                if resp.status != 200:
                    error_text = await resp.text()
                    raise aiohttp.ClientError(f"Ollama API è¿”å›é”™è¯¯çŠ¶æ€ {resp.status}: {error_text}")

                # ä½¿ç”¨ aiohttp çš„å¼‚æ­¥è¡Œè¯»å–å™¨
                async for line in resp.content:
                    # è·³è¿‡ç©ºè¡Œï¼ˆSSE å¸¸è§ï¼‰
                    line = line.strip()
                    if not line:
                        continue

                    try:
                        # è§£æ JSON è¡Œ
                        data = json.loads(line.decode("utf-8"))

                        # Ollama æ¯æ¬¡è¿”å›çš„å­—æ®µï¼š{'model':..., 'created_at':..., 'response': ..., 'done': ...}
                        response_text = data.get("response", "")

                        # å¦‚æœæ˜¯ç»“æŸæ ‡å¿—ï¼Œå¯é€‰æ‹©è¿”å›å®Œæˆä¿¡æ¯æˆ–é™é»˜å¿½ç•¥ï¼ˆçœ‹éœ€æ±‚ï¼‰
                        if data.get("done", False):
                            # ä¾‹å¦‚å¯é¢å¤– yield æ€»ç»“ä¿¡æ¯
                            if "total_duration" in data:
                                print(
                                    f"\n[æ¨¡å‹æµå¼å®Œæˆ] è€—æ—¶: {data['total_duration'] / 1e9:.2f}s, token æ•°: {data.get('eval_count', '?')}")
                            break

                        # åª yield ç”Ÿæˆçš„å†…å®¹ï¼ˆéç»“æŸæ ‡è®°ï¼‰
                        if response_text:
                            yield response_text

                    except json.JSONDecodeError as e:
                        print(f"[è­¦å‘Š] æ— æ³•è§£ææµæ•°æ®è¡Œ: {line!r} | é”™è¯¯: {e}", file=sys.stderr)
                        continue

        except aiohttp.ClientError as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        except Exception as e:
            print(f"æœªé¢„æœŸé”™è¯¯: {e}", file=sys.stderr)


# ğŸ”§ ä½¿ç”¨ç¤ºä¾‹
async def main():
    print("æ­£åœ¨è¯·æ±‚ Ollama (LLaMA3)...", end="\n\n")

    async for chunk in stream_ollama(
            model="gpt-oss:20b",
            prompt="ç”¨ 3å¥è¯è§£é‡Šç‰›é¡¿ç¬¬ä¸€å®šå¾‹ï¼š"
    ):
        # å®æ—¶æ‰“å°ï¼ˆä¸æ¢è¡Œï¼Œåƒæ‰“å­—æœºæ•ˆæœï¼‰
        print(chunk, end="", flush=True)

    print("\nâœ… æµå¼ç”Ÿæˆå®Œæ¯•ï¼")


if __name__ == "__main__":
    asyncio.run(main())
